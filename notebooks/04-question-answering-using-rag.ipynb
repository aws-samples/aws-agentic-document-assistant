{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a557f2-928c-404a-80bb-c6d9970d037f",
   "metadata": {},
   "source": [
    "# 04 - Question answering using Retrieval Augmented Generation\n",
    "In this notebook we will be using the output of the `document_processing.ipynb` notebook to answer questions using retrieval augmented generation.\n",
    "\n",
    "Notebook overview:\n",
    "1. Load the PDFs as plaintext (output of `document_processing.ipynb`).\n",
    "2. Chunk the plaintext into a reasonable size, such that the LLMs context window can fit multiple chunks.\n",
    "3. Embed each of the chunks using an LLM and store the embeddings in a vector store.\n",
    "4. Answer a user question using the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edbcf1-2b17-40c1-a983-38603c3f3414",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb6206-6c73-4d33-8034-55f0480b37a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e72e75-5b72-4565-872f-5df636c61c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.__version__, botocore.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab31147-83b2-4660-ad31-ef484ebd2581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssm_client = boto3.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ef2b3-d7b3-4ea8-833e-29553dfb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_region_parameter = \"/AgenticLLMAssistant/bedrock_region\"\n",
    "bedrock_endpoint_parameter = \"/AgenticLLMAssistant/bedrock_endpoint\"\n",
    "s3_bucket_name_parameter = \"/AgenticLLMAssistant/AgentDataBucketParameter\"\n",
    "\n",
    "BEDROCK_REGION = ssm_client.get_parameter(Name=bedrock_region_parameter)\n",
    "BEDROCK_REGION = BEDROCK_REGION[\"Parameter\"][\"Value\"]\n",
    "\n",
    "S3_BUCKET_NAME = ssm_client.get_parameter(Name=s3_bucket_name_parameter)\n",
    "S3_BUCKET_NAME = S3_BUCKET_NAME[\"Parameter\"][\"Value\"]\n",
    "\n",
    "BEDROCK_REGION, S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a50c0-7772-4788-b0d2-2832afce55a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_MODEL_ID = \"anthropic.claude-v1\"\n",
    "LLM_MODEL_ID = \"anthropic.claude-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e761fe-d3c9-4b77-ad10-73545ee5a00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retry_config = Config(\n",
    "    region_name=BEDROCK_REGION, retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee5ee2-d095-420f-908a-6575cb47f832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock\", config=retry_config)\n",
    "bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd91be5-9ef1-4c8e-8a17-1e402bc65977",
   "metadata": {},
   "source": [
    "### Load the PDFs as plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68b1cb-0265-4fc9-bfcd-70ee9e85021b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import load_list_from_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e29ad-7976-44ed-aca4-177e89cad9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_processed = load_list_from_s3(S3_BUCKET_NAME, \"documents_processed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eab01d-6f87-4a14-aec4-604c1bd2921c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf059903-ad05-4abf-af67-378094c4de5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will be using Langchains vectorstores to store the embeddings, therefore, we will convert the documents to Langchain Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a0301-b374-462c-bf72-18aeeaf828b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "langchain_documents_text = []\n",
    "langchain_documents_tables = []\n",
    "\n",
    "for document in documents_processed:\n",
    "    document_name = document[\"name\"]\n",
    "    document_source_location = document[\"source_location\"]\n",
    "    document_s3_metadata = document[\"metadata\"]\n",
    "    print(document_s3_metadata)\n",
    "    mapping_to_original_page_numbers = {\n",
    "        idx: pg_num\n",
    "        for idx, pg_num in enumerate(json.loads(document_s3_metadata[\"pages_kept\"]))\n",
    "    }\n",
    "    # remove pages_kept since we already put the original page number.\n",
    "    del document_s3_metadata[\"pages_kept\"]\n",
    "\n",
    "    for page in document[\"pages\"]:\n",
    "        # Turn each page into a Langchain Document.\n",
    "        # Note: you could choose to also prepend part of the previous document\n",
    "        # and append part of the next document to include more context for documents\n",
    "        # that have many pages which continue their text on the next page.\n",
    "        current_metadata = {\n",
    "            \"document_name\": document_name,\n",
    "            \"document_source_location\": document_source_location,\n",
    "            \"page_number\": page[\"page\"],\n",
    "            \"original_page_number\": mapping_to_original_page_numbers[page[\"page\"]],\n",
    "        }\n",
    "        # merge the document_s3_metadata into the langchain Document metadata\n",
    "        # to be able to use them for filtering.\n",
    "        current_metadata.update(document_s3_metadata)\n",
    "\n",
    "        langchain_documents_text.append(\n",
    "            Document(page_content=page[\"page_text\"], metadata=current_metadata)\n",
    "        )\n",
    "\n",
    "        # Turn all the tables of the pages into seperate Langchain Documents as well\n",
    "        for table_markdown in page[\"page_tables\"]:\n",
    "            langchain_documents_tables.append(\n",
    "                Document(page_content=table_markdown, metadata=current_metadata)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8594299-e64a-4857-9ac9-4a27981a0f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Your documents contain {len(langchain_documents_text)}\"\n",
    "    f\" pages and {len(langchain_documents_tables)} tables combined\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110d75c-93af-4524-ab36-d892ad5f06ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text), len(langchain_documents_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93116cc2-d831-4f02-bec4-294b6cb79435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_lengths = [len(doc.page_content) for doc in langchain_documents_text]\n",
    "\n",
    "print(\n",
    "    f\"Number of chunks: {len(chunk_lengths)},\"\n",
    "    f\" Max chunk length {max(chunk_lengths)},\"\n",
    "    f\" Min chunk length: {min(chunk_lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d5e51-d36e-40cb-a7df-203827afbb56",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot the distribution of the length of the smaller chunks, to determine if some of them should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dfc8d-7fbc-40d8-9208-a2cd26662030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_chunks_length_distribution(chunks, bin_size=50):\n",
    "    chunk_lengths = [len(doc.page_content) for doc in chunks]\n",
    "    bin_size = 50\n",
    "    num_bins = (max(chunk_lengths) - min(chunk_lengths)) // bin_size + 1\n",
    "\n",
    "    # Display smoothed histogram using seaborn\n",
    "    sns.histplot(chunk_lengths, kde=True, bins=num_bins, edgecolor=\"black\")\n",
    "    plt.xlabel(\"Chunk Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Chunk Lengths\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493f2fd-87af-4176-8564-ffec4d0b374b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_chunks_length_distribution(langchain_documents_text, bin_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402cd24-5993-44fa-b65d-ae3ca53ce4e8",
   "metadata": {},
   "source": [
    "### Chunk the Documents containing page text and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eff03-e018-4692-86f5-9d64d484cc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2454dc-8228-4aa1-b201-52df780e43a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# The chunk overlap duplicates some text across chunks to prevent context from being lost between chunks.\n",
    "# TODO: the following spliting uses tiktoken, create a custom one that use the tokenizer from anthropic.\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eafc5-e907-4687-b000-2cf991aa43d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_documents_text_chunked = text_splitter.split_documents(\n",
    "    langchain_documents_text + langchain_documents_tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f290d03-23ed-4be3-8ed1-f89af5a1a197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ad0e9-708d-4e0d-b8b0-dc5862522011",
   "metadata": {},
   "source": [
    "Due to splitting of the chunks, we may have created some small chunks that are already captured in the chunk overlap.<br>\n",
    "It is recommended to have a look and estimate until which chunk length, the chunks can be removed.<br>\n",
    "The reason for this is that semantic search can occasionally favor chunks containing very little content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c5edf-ed17-4ff9-909a-7689937c5568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_chunks_length_distribution(langchain_documents_text_chunked, bin_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01045e-abff-4bfe-9603-93e31f77ce65",
   "metadata": {},
   "source": [
    "### Embed the documents and store them in a vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242869f7-1628-45cd-9909-1c3777210b75",
   "metadata": {},
   "source": [
    "For the embedding model we use Amazon Titan embedding model available through Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30873eab-a237-4694-b767-13df5cb5123d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock as LangchainBedrock\n",
    "\n",
    "# Define an embedding model to generate embeddings\n",
    "# TODO: request access for other embedding models\n",
    "embedding_model_id = \"amazon.titan-embed-text-v1\"\n",
    "embedding_model = BedrockEmbeddings(model_id=embedding_model_id, client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8aebe-703b-4692-a3ca-f3608c44fe9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: The following lines of code create embedding for each document chunk and store them, this will use your embedding model for each document and incur costs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2d594-b1b4-4ae2-bab1-2ee027178701",
   "metadata": {},
   "source": [
    "TODO - implement the following improvements to the embedding generation process:\n",
    "\n",
    "* Decorrelate embedding generation from `FAISS.from_documents`.\n",
    "* Create the embedding independently and save them on S3, then update the code to attempt to load them from S3 before calling the embedding model.\n",
    "* Create the index from the embeddings and text pair, without calling the embedding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dec29a-e893-407b-bf9b-eaf1d8950b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e7bb-28f6-4134-a3ef-9c716017c9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss = FAISS.from_documents(langchain_documents_text_chunked, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bf2b0-4982-4f43-8535-3b421aa65693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faiss.save_local(\"faiss_vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b3d58-b8ed-40d2-a8fc-954beb408741",
   "metadata": {},
   "source": [
    "### Answer questions using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125089c-7867-45c6-a75c-247a5a27022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define a custom template for structured data extraction\n",
    "template = \"\"\"\\n\\nHuman:\n",
    "The following is a friendly conversation between a human and an AI assistant.\n",
    "The assistant is helpful and answers questions about documents based on extracts of those documents available within the <context></context> XML tags.\n",
    "When answering, the assistant abides by the rules defined in the <rules></rules> XML tags.\n",
    "\n",
    "The rules are below:\n",
    "<rules>\n",
    "The assistant answers the human questions accurately and concisely.\n",
    "When answering, the assistant relies on the context available in the <context></context> XML tags below.\n",
    "The assistant considers the conversation history available in the <history></history> XML tags when answering.\n",
    "If the assistant does not know the answer to a question, it truthfully says it does not know.\n",
    "The assistant answers in Markdown and puts the answer inside <response></response> XML tags.\n",
    "</rules>\n",
    "\n",
    "The retrieved document chuncks that the AI assistant uses to respond are below within the <context></context> XML tags, each document chunk is within a <document></document> XML tags:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Current conversation:\n",
    "<history>\n",
    "{history}\n",
    "</history>\n",
    "\n",
    "The user input to respond to is within the input XML tags below:\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\\n\\nAssistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"context\"], template=template\n",
    ")\n",
    "\n",
    "claude_llm = LangchainBedrock(\n",
    "    model_id=LLM_MODEL_ID,\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 512, \"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=claude_llm,\n",
    "    memory=ConversationBufferMemory(input_key=\"input\"),\n",
    "    prompt=PROMPT,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e5c30-8adc-4853-99fb-0f2607dc6d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_documents(retrieved_documents):\n",
    "    combined_documents = \"\"\n",
    "    # Add each of the extracts to the query\n",
    "    for idx, extract in enumerate(retrieved_documents):\n",
    "        combined_documents += f\"<document>{extract.page_content}</document>\\n\"\n",
    "\n",
    "    return combined_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4d8a0-510f-4768-8816-427943587c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"Who were in the board of directors of Amazon in 2021 and what were their positions?\"\n",
    "\n",
    "k = 4\n",
    "retrieved_chuncks = faiss.similarity_search(query=query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb89fd8-d6b5-458f-8148-8a4d8bbb4e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = conversation.predict(\n",
    "    input=query, context=combine_documents(retrieved_chuncks)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14de68-5bda-40ee-854f-c324dab6ab6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = response.strip().strip(\"<response>\").strip(\"</response>\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc849ee7-54fa-498d-85b8-e7549eb9459c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f66e8-4d1c-4785-80d4-3c6824873a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload semantic search index to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f213cc-b612-47ef-a07f-2b1e5863ef23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync ./faiss_vector_store s3://{S3_BUCKET_NAME}/semantic_search_index/faiss_vector_store/"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
